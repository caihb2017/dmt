# 1.分类

**输出变量y只能接受少数几个可能的值中的一个**

例子：

| 问题                       | 回答  |
| -------------------------- | ----- |
| 电子邮件是否垃圾邮件       | 是 否 |
| 在线金融交易是否是欺诈性的 | 是 否 |
| 肿瘤归类是否为恶性肿瘤     | 是 否 |

这种只有两个可能性输出的分类问题称为**二进制分类**

引用这两个类或范畴常见的方式：

> false true
>
> 0       1

0：消极类	1：积极类

> 消极和积极并不意味着坏与好，只是用否定的和肯定的例子



# 2.逻辑回归

例子：

*肿瘤是否恶性的分类*

<img src="./image/1.jpg" style="zoom:50%;" />

> 1代表恶性肿瘤	0代表无或阴性

logistic回归最终要做的是拟合一条曲线



---

**逻辑函数**

> 一种将变量映射到到[ 0 , 1 ]之间的函数

<img src="./image/2.jpg" style="zoom:50%;" />

---



## 2.1构建逻辑回归函数

<img src="image/3.jpg" style="zoom:50%;" />

将这两个等式放在一起，得到：

<img src="image/4.jpg" style="zoom:50%;" />

这就是logistic回归模型
$$
f_{\vec{w},b}(\vec{x})=0.7
$$

> 代表有百分之七十的记录y等于1

$$
f_{\vec{w},b}(\vec{x})=P(y=1|\vec{x};\vec{w},b)
$$

# 3.决策边界

让学习算法预测y的值：设置一个阈值

> 阈值之上预测y是1

一个通常的方式是选择0.5的阈值

> if f>=0.5:
>
> ​	y=1
>
> else:
>
> ​	y=0

<img src="image/5.jpg" style="zoom:50%;" />

## 例子：

<img src="image/6.jpg" style="zoom:50%;" />

对于上述例子有函数：

<img src="image/7.jpg" style="zoom:50%;" />

>在上述例子中w1=1，w2=1，b=-3

**决策边界：**
$$
z=\vec{w}*\vec{x}+b=0
$$

> 在上述例子中
>
> z=x1+x2-3=0
>
> ​	x1+x2=3

## 一个更复杂的例子：

<img src="image/8.jpg" style="zoom:50%;" />

在这个例子中，我们设置z：
$$
z=w_{1}x_{1}^2+w_{2}x_{2}^2+b
$$

>w1=1，w2=1，b=-1

**决策边界：**
$$
z=x_{1}^2+x_{2}^2-1=0
$$
画在图中如图所示：

<img src="image/9.jpg" style="zoom:50%;" />

> 当z>=0,y=1	圆圈外区域
>
> 当z<0,y=0	圆圈内区域



---

更高阶的多项式：形状内y=0，形状外y=1

如果没有这些高阶多项式，比如使用的特征只用x1、x2、x3等，那么逻辑回归的决策边界永远时线性的，永远是一条直线。

# 4.逻辑回归的代价函数

<u>平方误差成本函数不是逻辑回归的理想成本函数</u>

下面是平方误差成本函数：

<img src="image/12.jpg" style="zoom:40%;" />

对于线性函数来说：梯度下降在全局最小值收敛

<img src="image/10.jpg" style="zoom:50%;" />

对于逻辑回归函数：可能会陷入很多局部最小值

<img src="image/11.jpg" style="zoom:50%;" />

---

对数损失函数（logistic loss function）

<img src="image/13.jpg" style="zoom:40%;" />

* 当y=1时,

  <img src="image/14.jpg" style="zoom:45%;" />

* 当y=0时

<img src="image/15.jpg" style="zoom:50%;" />

**成本函数**

<img src="image/16.jpg" style="zoom:45%;" />

## 4.1简化版成本函数

简化版损失函数

<img src="image/17.jpg" style="zoom:50%;" />

简化版代价函数

<img src="image/18.jpg" style="zoom:50%;" />

# 5.梯度下降实现

w,b的求解

<img src="image/20.jpg" style="zoom:50%;" />

# 6.过拟合的问题

线性回归的例子：对比右边的模型，中间的模型给出了更合理的预测

<img src="image/21.jpg" style="zoom:50%;" />

机器学习的目的是找一个模型适合并且不会矫枉过正

分类：

<img src="image/22.png" style="zoom:50%;" />

## 6.1解决过拟合的问题

收集更多的数据：学习算法将拟合一个不那么摆动的函数

<img src="image/23.jpg" style="zoom:50%;" />

不使用过多的特征

正则化：保留所有的功能，防止功能产生过大的影响(减小参数大小)

<img src="image/24.jpg" style="zoom:50%;" />

## 6.2正则化代价函数

<img src="image/25.jpg" style="zoom:50%;" />

## 6.3正则化线性回归

<img src="image/26.jpg" style="zoom:50%;" />

## 6.4正则化逻辑回归

<img src="image/27.jpg" style="zoom:50%;" />

<img src="image/28.jpg" style="zoom:50%;" />
